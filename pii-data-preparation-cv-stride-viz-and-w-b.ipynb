{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for PII Data Detection\n",
    "\n",
    "This notebook shares my current approach to CV, striding, visualization and dataset versioning with W&B. \n",
    "\n",
    "You may want to run it interactively or add W&B API key to the secrets to run it offline.\n",
    "\n",
    "You can check out [the video from my live training session](https://www.youtube.com/watch?v=w4ZDwiSXMK0).\n",
    "\n",
    "I also saved the outputs to the [Kaggle dataset](https://www.kaggle.com/datasets/thedrcat/pii-detection-cv-split) if you want to import it in a Kaggle training notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6807"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "train = json.load(open(\"data_dir/train.json\"))\n",
    "df = pd.DataFrame(train)\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Split\n",
    "\n",
    "Let's start by checking out the distribution of labels across all training essays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL: 24\n",
      "ID_NUM: 33\n",
      "NAME_STUDENT: 891\n",
      "PHONE_NUM: 4\n",
      "STREET_ADDRESS: 2\n",
      "URL_PERSONAL: 72\n",
      "USERNAME: 5\n",
      "OTHER: 5862\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(df):\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda x: list(set(\n",
    "        [l.split('-')[1] for l in x if l != 'O']\n",
    "         )))\n",
    "    # add 1-hot encoding\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    \n",
    "    # add 'OTHER' column\n",
    "    df['OTHER'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    \n",
    "    return df, list(mlb.classes_) + ['OTHER']\n",
    "\n",
    "df, label_classes = encode_labels(df)\n",
    "\n",
    "for col in label_classes:\n",
    "    print(f'{col}: {df[col].sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want all the very rare classes to be in my validation split. This is going to be an opinionated split, but I'd like to pick the following numbers into my validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID EMAIL: 13\n",
      "TRAIN EMAIL: 11\n",
      "VALID ID_NUM: 13\n",
      "TRAIN ID_NUM: 20\n",
      "VALID NAME_STUDENT: 124\n",
      "TRAIN NAME_STUDENT: 767\n",
      "VALID PHONE_NUM: 4\n",
      "TRAIN PHONE_NUM: 0\n",
      "VALID STREET_ADDRESS: 2\n",
      "TRAIN STREET_ADDRESS: 0\n",
      "VALID URL_PERSONAL: 26\n",
      "TRAIN URL_PERSONAL: 46\n",
      "VALID USERNAME: 5\n",
      "TRAIN USERNAME: 0\n",
      "VALID OTHER: 1000\n",
      "TRAIN OTHER: 4862\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Create a 'valid' column and set it to False\n",
    "df['valid'] = False\n",
    "\n",
    "# Define the validation numbers\n",
    "val_nums = {\n",
    "    'EMAIL': 12,\n",
    "    'ID_NUM': 12,\n",
    "    'NAME_STUDENT': 100,\n",
    "    'PHONE_NUM': 4,\n",
    "    'STREET_ADDRESS': 2,\n",
    "    'URL_PERSONAL': 20,\n",
    "    'USERNAME': 5,\n",
    "    'OTHER': 1000, \n",
    "}\n",
    "\n",
    "# For each class in val_nums, randomly select the specified number of examples and set 'valid' to True\n",
    "for label, num in val_nums.items():\n",
    "    valid_indices = df[df[label] == 1].sample(n=num, random_state=42).index\n",
    "    df.loc[valid_indices, 'valid'] = True\n",
    "\n",
    "\n",
    "# Let's double check the classes per split:\n",
    "for col in label_classes:\n",
    "    print(f'VALID {col}: {df[df.valid == True][col].sum()}')\n",
    "    print(f'TRAIN {col}: {df[df.valid == False][col].sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Let's prepare the visualization code based on [this great notebook](https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "options = {\n",
    "    \"colors\": {\n",
    "        \"B-NAME_STUDENT\": \"aqua\",\n",
    "        \"I-NAME_STUDENT\": \"skyblue\",\n",
    "        \"B-EMAIL\": \"limegreen\",\n",
    "        \"I-EMAIL\": \"lime\",\n",
    "        \"B-USERNAME\": \"hotpink\",\n",
    "        \"I-USERNAME\": \"lightpink\",\n",
    "        \"B-ID_NUM\": \"purple\",\n",
    "        \"I-ID_NUM\": \"rebeccapurple\",\n",
    "        \"B-PHONE_NUM\": \"red\",\n",
    "        \"I-PHONE_NUM\": \"salmon\",\n",
    "        \"B-URL_PERSONAL\": \"silver\",\n",
    "        \"I-URL_PERSONAL\": \"lightgray\",\n",
    "        \"B-STREET_ADDRESS\": \"brown\",\n",
    "        \"I-STREET_ADDRESS\": \"chocolate\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def visualize(row):\n",
    "    doc = nlp(row.full_text)\n",
    "    doc.ents = [\n",
    "        Span(doc, idx, idx + 1, label=label)\n",
    "        for idx, label in enumerate(row.labels)\n",
    "        if label != \"O\"\n",
    "    ]\n",
    "    html = displacy.render(doc, style=\"ent\", jupyter=False, options=options)\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# html = visualize(df.loc[0])\n",
    "# display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation with stride\n",
    "\n",
    "There are two ways to do striding here - the best is probably to use tokenizers striding method. I opted for the easy way here and applied striding using spacy tokens. This means we're still facing variable sequence length after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document                                                           17809\n",
       "full_text              Assignment: Mindmapping\\n\\nChallenge\\n\\nThe ch...\n",
       "tokens                 [Assignment, :, Mindmapping, \\n\\n, Challenge, ...\n",
       "trailing_whitespace    [False, True, False, False, False, False, True...\n",
       "labels                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "unique_labels                                                         []\n",
       "EMAIL                                                                  0\n",
       "ID_NUM                                                                 0\n",
       "NAME_STUDENT                                                           0\n",
       "PHONE_NUM                                                              0\n",
       "STREET_ADDRESS                                                         0\n",
       "URL_PERSONAL                                                           0\n",
       "USERNAME                                                               0\n",
       "OTHER                                                                  1\n",
       "valid                                                              False\n",
       "token_indices          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "Name: 4624, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_indices(doc_tokens):\n",
    "    token_indices = list(range(len(doc_tokens)))\n",
    "    return token_indices\n",
    "\n",
    "df['token_indices'] = df['tokens'].apply(add_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:33:30.805994Z",
     "iopub.status.busy": "2024-02-09T13:33:30.805559Z",
     "iopub.status.idle": "2024-02-09T13:33:30.824196Z",
     "shell.execute_reply": "2024-02-09T13:33:30.823204Z",
     "shell.execute_reply.started": "2024-02-09T13:33:30.805964Z"
    }
   },
   "outputs": [],
   "source": [
    "def rebuild_text(tokens, trailing_whitespace):\n",
    "    text = ''\n",
    "    for token, ws in zip(tokens, trailing_whitespace):\n",
    "        ws = \" \" if ws == True else \"\"\n",
    "        text += token + ws\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_rows(df, max_length, doc_stride):\n",
    "    new_df = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        if len(tokens) > max_length:\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                remaining_tokens = len(tokens) - start\n",
    "                if remaining_tokens < max_length and start != 0:\n",
    "                    # Adjust start for the last window to ensure it has max_length tokens\n",
    "                    start = max(0, len(tokens) - max_length)\n",
    "                end = min(start + max_length, len(tokens))\n",
    "                new_row = {}\n",
    "                new_row['document'] = row['document']\n",
    "                new_row['valid'] = row['valid']\n",
    "                new_row['tokens'] = tokens[start:end]\n",
    "                new_row['trailing_whitespace'] = row['trailing_whitespace'][start:end]\n",
    "                new_row['labels'] = row['labels'][start:end]\n",
    "                new_row['token_indices'] = list(range(start, end))\n",
    "                new_row['full_text'] = rebuild_text(new_row['tokens'], new_row['trailing_whitespace'])\n",
    "                new_df.append(new_row)\n",
    "                if remaining_tokens >= max_length:\n",
    "                    start += doc_stride\n",
    "                else:\n",
    "                    # Break the loop if we've adjusted for the last window\n",
    "                    break\n",
    "        else:\n",
    "            new_row = {\n",
    "                'document': row['document'], \n",
    "                'valid': row['valid'],\n",
    "                'tokens': row['tokens'], \n",
    "                'trailing_whitespace': row['trailing_whitespace'], \n",
    "                'labels': row['labels'], \n",
    "                'token_indices': row['token_indices'], \n",
    "                'full_text': row['full_text']\n",
    "            }\n",
    "            new_df.append(new_row)\n",
    "    return pd.DataFrame(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:33:31.387494Z",
     "iopub.status.busy": "2024-02-09T13:33:31.387110Z",
     "iopub.status.idle": "2024-02-09T13:33:35.746754Z",
     "shell.execute_reply": "2024-02-09T13:33:35.745318Z",
     "shell.execute_reply.started": "2024-02-09T13:33:31.387466Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 750\n",
    "doc_stride = 250\n",
    "stride_df = split_rows(df, max_length, doc_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:33:38.199735Z",
     "iopub.status.busy": "2024-02-09T13:33:38.199329Z",
     "iopub.status.idle": "2024-02-09T13:33:38.207354Z",
     "shell.execute_reply": "2024-02-09T13:33:38.206098Z",
     "shell.execute_reply.started": "2024-02-09T13:33:38.199706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 11468)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(stride_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:33:49.156079Z",
     "iopub.status.busy": "2024-02-09T13:33:49.155654Z",
     "iopub.status.idle": "2024-02-09T13:33:49.557650Z",
     "shell.execute_reply": "2024-02-09T13:33:49.556532Z",
     "shell.execute_reply.started": "2024-02-09T13:33:49.156050Z"
    }
   },
   "outputs": [],
   "source": [
    "stride_df, label_classes = encode_labels(stride_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to W&B\n",
    "\n",
    "It's best practice to version datasets properly and visualize them in W&B. Let's do this!\n",
    "\n",
    "To run below code, please add your `WANDB_API_KEY` secret to Kaggle notebook secrets. You can get it [here](https://wandb.ai/authorize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:35:51.844302Z",
     "iopub.status.busy": "2024-02-09T13:35:51.843764Z",
     "iopub.status.idle": "2024-02-09T13:36:28.132004Z",
     "shell.execute_reply": "2024-02-09T13:36:28.130703Z",
     "shell.execute_reply.started": "2024-02-09T13:35:51.844267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_133556-ku2o5r8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/darek/pii/runs/ku2o5r8l' target=\"_blank\">exalted-hill-61</a></strong> to <a href='https://wandb.ai/darek/pii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/darek/pii' target=\"_blank\">https://wandb.ai/darek/pii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/darek/pii/runs/ku2o5r8l' target=\"_blank\">https://wandb.ai/darek/pii/runs/ku2o5r8l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/darek/pii/runs/ku2o5r8l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7c140f133700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "wandb.init(project='pii', job_type='preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:36:28.136203Z",
     "iopub.status.busy": "2024-02-09T13:36:28.134712Z",
     "iopub.status.idle": "2024-02-09T13:36:28.622474Z",
     "shell.execute_reply": "2024-02-09T13:36:28.621065Z",
     "shell.execute_reply.started": "2024-02-09T13:36:28.136155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's add our hyperparameters to the config \n",
    "\n",
    "wandb.config.update({\n",
    "    'max_length': max_length,\n",
    "    'doc_stride': doc_stride,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:36:54.464701Z",
     "iopub.status.busy": "2024-02-09T13:36:54.464200Z",
     "iopub.status.idle": "2024-02-09T13:37:06.015645Z",
     "shell.execute_reply": "2024-02-09T13:37:06.014373Z",
     "shell.execute_reply.started": "2024-02-09T13:36:54.464645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact processed_data>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first log data as artifacts\n",
    "\n",
    "df.to_parquet('raw_data.parquet', index=False)\n",
    "stride_df.to_parquet('stride_data.parquet', index=False)\n",
    "\n",
    "raw_data = wandb.Artifact(name=\"raw_data\", type=\"dataset\")\n",
    "raw_data.add_file('raw_data.parquet')\n",
    "wandb.log_artifact(raw_data)\n",
    "\n",
    "processed_data = wandb.Artifact(name=\"processed_data\", type=\"dataset\")\n",
    "processed_data.add_file('stride_data.parquet')\n",
    "wandb.log_artifact(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T13:39:23.147548Z",
     "iopub.status.busy": "2024-02-09T13:39:23.147056Z",
     "iopub.status.idle": "2024-02-09T13:46:56.235958Z",
     "shell.execute_reply": "2024-02-09T13:46:56.234661Z",
     "shell.execute_reply.started": "2024-02-09T13:39:23.147512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    }
   ],
   "source": [
    "# We will generate html viz for every train essay, wrap it up in `wandb.Html` and create a W&B table to inspect it\n",
    "wandb_htmls = [wandb.Html(visualize(row)) for _, row in df.iterrows()]\n",
    "df['visualization'] = wandb_htmls\n",
    "table = wandb.Table(dataframe=df)\n",
    "wandb.log({'original_dataset': table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771c804215bf443cb4bd9108a7da7f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='30.090 MB of 30.090 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-monkey-34</strong> at: <a href='https://wandb.ai/darek/pii/runs/46cown4z' target=\"_blank\">https://wandb.ai/darek/pii/runs/46cown4z</a><br/> View job at <a href='https://wandb.ai/darek/pii/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNzYwNDI4Nw==/version_details/v5' target=\"_blank\">https://wandb.ai/darek/pii/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNzYwNDI4Nw==/version_details/v5</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240207_235806-46cown4z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Share your findings\n",
    "\n",
    "If you find some good insights from inspecting the data, please share in the comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
